{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d026da78-556b-4a73-933d-0812f33ee414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 images for Afnan\n",
      "Loaded 9 images for rumana\n",
      "Loaded 8 images for Shreya\n",
      "Loaded 5 images for Smiksha\n",
      "\n",
      "Loaded 4 persons.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
    "\n",
    "# Paths\n",
    "KNOWN_FACES_DIR = r\"C:\\Users\\afnan\\Desktop\\known_faces\"\n",
    "SIMILARITY_THRESHOLD = 0.6\n",
    "MAX_FACES_PER_FRAME = 5\n",
    "\n",
    "# Load known faces\n",
    "known_faces = {}\n",
    "for person in os.listdir(KNOWN_FACES_DIR):\n",
    "    person_dir = os.path.join(KNOWN_FACES_DIR, person)\n",
    "    if os.path.isdir(person_dir):\n",
    "        embeddings = []\n",
    "        for img_file in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            boxes, _ = mtcnn.detect(img_rgb)\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                box = boxes[0]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face = img[y1:y2, x1:x2]\n",
    "                if face.size == 0:\n",
    "                    continue\n",
    "                face_resized = cv2.resize(face, (160, 160))\n",
    "                face_resized = face_resized.astype('float32') / 255.0\n",
    "                face_tensor = torch.from_numpy(face_resized.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    emb = resnet(face_tensor).cpu().numpy()\n",
    "                embeddings.append(emb.flatten())\n",
    "        if embeddings:\n",
    "            mean_emb = np.mean(embeddings, axis=0)\n",
    "            known_faces[person] = {'embedding': mean_emb, 'count': len(embeddings)}\n",
    "            print(f\"Loaded {len(embeddings)} images for {person}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(known_faces)} persons.\")\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(frame_rgb)\n",
    "    \n",
    "    if boxes is not None:\n",
    "        unknown_embeddings = []\n",
    "\n",
    "        for box in boxes[:MAX_FACES_PER_FRAME]:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            if face.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Face recognition processing\n",
    "            face_resized = cv2.resize(face, (160, 160))\n",
    "            face_resized = face_resized.astype('float32') / 255.0\n",
    "            face_tensor = torch.from_numpy(face_resized.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                emb = resnet(face_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            # Check for duplicate faces\n",
    "            is_duplicate = False\n",
    "            for known_emb in unknown_embeddings:\n",
    "                if cosine_similarity([emb], [known_emb])[0][0] > 0.8:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                name = \"Unknown\"\n",
    "                best_score = -1\n",
    "                for person, data in known_faces.items():\n",
    "                    score = cosine_similarity([emb], [data['embedding']])[0][0]\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        if score >= SIMILARITY_THRESHOLD:\n",
    "                            name = person\n",
    "\n",
    "                if name == \"Unknown\":\n",
    "                    unknown_embeddings.append(emb)\n",
    "\n",
    "                # Emotion detection using DeepFace\n",
    "                emotion = \"N/A\"\n",
    "                try:\n",
    "                    face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "                    if face_rgb.shape[0] >= 64 and face_rgb.shape[1] >= 64:\n",
    "                        emotion_result = DeepFace.analyze(\n",
    "                            face_rgb, \n",
    "                            actions=['emotion'],\n",
    "                            detector_backend='opencv',\n",
    "                            enforce_detection=False,\n",
    "                            silent=True\n",
    "                        )\n",
    "                        emotion = emotion_result[0]['dominant_emotion']\n",
    "                except Exception as e:\n",
    "                    print(f\"Emotion detection warning: {str(e)}\")\n",
    "\n",
    "                # Draw rectangle and label\n",
    "                color = (0, 0, 255) if name == \"Unknown\" else (0, 255, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f\"{name} ({best_score:.2f}) - {emotion}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    cv2.imshow('Face Recognition & Emotion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa8f4c-0d8e-498b-82b7-e74158a9ecd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
